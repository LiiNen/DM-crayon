{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from konlpy.tag import  Okt\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 월별 데이터로 생성하기\n",
    "json_list = os.listdir('../news-crawler/it/')\n",
    "json_files = [file for file in json_list if file.endswith('.json')]  \n",
    "data = []\n",
    "df = pd.DataFrame()\n",
    "for i in json_files:\n",
    "    for line in open(('../news-crawler/it/'+i),\"r\", encoding='utf-8-sig'):\n",
    "        df = pd.concat([df, pd.DataFrame(json.loads(line), columns=['id', 'title', 'content', 'date', 'like'])])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 주차별 데이터로 생성하기\n",
    "# with open(\"../news-crawler/it/2021-11-07-20.json\", \"r\", encoding='utf-8-sig') as f:\n",
    "#     tmp = json.load(f)\n",
    "# df = pd.DataFrame(json.loads(line), columns=['id', 'title', 'content', 'date', 'like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt() \n",
    "# okt.analyze  #구(Phrase) 분석\n",
    "# okt.morphs   #형태소 분석\n",
    "# okt.nouns    #명사 분석\n",
    "# okt.pos      #형태소 분석 태깅\n",
    "\n",
    "noun_list = []\n",
    "for content in tqdm(df['content']): \n",
    "    nouns = okt.nouns(content)\n",
    "    noun_list.append(nouns)\n",
    "df['nouns'] = noun_list\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 명사 집합으로 보고 문서 리스트로 치환 (tfidfVectorizer 인풋 형태를 맞추기 위해)\n",
    "text = [\" \".join(noun) for noun in df['nouns']]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 5, ngram_range=(1,5))\n",
    "tfidf_vectorizer.fit(text)\n",
    "vector = tfidf_vectorizer.transform(text).toarray()\n",
    "\n",
    "vector = np.array(vector) # Normalizer를 이용해 변환된 벡터\n",
    "model = DBSCAN(eps=0.3,min_samples=6, metric = \"cosine\")\n",
    "# 거리 계산 식으로는 Cosine distance를 이용\n",
    "result = model.fit_predict(vector)\n",
    "df['result'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete noise or garbage data\n",
    "df = df[df['result'] != -1]\n",
    "df = df[df['result'] != 0]\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "# Create clustering using titles\n",
    "for i in df['result'].unique().tolist():\n",
    "    cluster_dict[i] = df[df['result'] == i].title.tolist()\n",
    "\n",
    "# Order by Hot topic    \n",
    "sorted_cluster_dict = sorted(cluster_dict.items(), key=lambda x : len(x[1]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n",
    "\n",
    "# Create 10 data sets of clustered data\n",
    "for idx, (cluster_num, titles) in enumerate(sorted_cluster_dict[:10]):\n",
    "    keywords_list = []\n",
    "    id_list = []\n",
    "    like_list = []\n",
    "\n",
    "    # Extract keyword from title\n",
    "    for title in titles:\n",
    "        keywords = okt.nouns(title)\n",
    "        keywords_list.extend(keywords)\n",
    "        # Add like / ID for sorting and linking\n",
    "        id_list = df[df['result']==cluster_num].id.tolist()\n",
    "        like_list = df[df['result']==cluster_num].like.tolist()\n",
    "    # Create keyword standard\n",
    "    standard_keyword = set(keywords_list)\n",
    "\n",
    "    tmp_keyword = []\n",
    "\n",
    "    # Keyword Counting\n",
    "    for keyword in standard_keyword:\n",
    "        tmp_keyword.append((keyword, keywords_list.count(keyword)))\n",
    "\n",
    "    # Hot Keyword Top 3\n",
    "    tmp_keyword.sort(key = lambda x : x[1], reverse=True)\n",
    "    final_keyword = [keyword for keyword in tmp_keyword[:3]]\n",
    "\n",
    "    # Create dictonary file\n",
    "    final_data = []\n",
    "    for id, title, like in zip(id_list, titles, like_list):\n",
    "        id['like'] = like\n",
    "        id['title'] = title\n",
    "        final_data.append(id)\n",
    "    final_dict[f\"cluster{idx}\"] = {}\n",
    "    final_dict[f'cluster{idx}']['keyword'] = final_keyword\n",
    "    final_dict[f'cluster{idx}']['data'] = final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
